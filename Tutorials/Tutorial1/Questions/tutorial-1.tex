% \documentclass[a4paper]{siamart220329}
\documentclass[a4paper]{siamonline220329}
\usepackage{kbordermatrix}

\usepackage{damacros}

% Theorem environment for questions: Small-caps header, italize body.
\theoremstyle{plain}
% \theoremheaderfont{\normalfont\sc}
\theoremheaderfont{\normalfont\bf}
\theorembodyfont{\normalfont}
\theoremseparator{.}
\theoremsymbol{}
\newtheorem{question}{Question}

% Sets running headers as well as PDF title and authors
\headers{Turing-like bifurcations for neural fields}{D. Avitabile}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Tutorial on Turing-like bifurcations for neural fields} 

% Authors: full names plus addresses.
\author{%
  Daniele Avitabile%
  \thanks{%
    Vrije Universiteit Amsterdam,
    Department of Mathematics,
    Faculteit der Exacte Wetenschappen,
    De Boelelaan 1081a,
    1081 HV Amsterdam, The Netherlands.
  \protect
    MathNeuro Team,
    Inria branch of the University of Montpellier,
    860 rue Saint-Priest
    34095 Montpellier Cedex 5
    France.
  \protect
    (\email{d.avitabile@vu.nl}, \url{www.danieleavitabile.com}).
  }
  % \and
%   Paul T. Frank \thanks{Department of Applied Mathematics, Fictional University, Boise, ID 
% (\email{ptfrank@fictional.edu}, \email{jesmith@fictional.edu}).}
% \and Jane E. Smith\footnotemark[3]
}

\begin{document}

\maketitle

\begin{abstract}
  In this tutorial we discuss a basic mechanism for the formation of spatial patterns
  from homogeneous steady states in cortical models knows as Neural Field Equations.
  We use a mix of formal nonlinear analysis and numerical simulations. Before
  starting this tutorial download the latest version of \href{https://zenodo.org/records/11120604}{the accompanying
  dataset}. Solutions to the exercises can be 
  \href{http://htmlpreview.github.io/?https://github.com/danieleavitabile/numerical-analysis-mathematical-neuroscience/blob/main/Tutorials/Tutorial1/Solutions/html/driver.html}{browsed here}
  and code can be downloaded from
  \href{https://github.com/danieleavitabile/numerical-analysis-mathematical-neuroscience/tree/main}{our
  repository}.
\end{abstract}

\section{Introduction}\label{sec:intro} 
We will study the emergence of spatially-periodic stationary solutions in the
following Neural Field Equation (NFE)
\begin{equation}\label{eq:NF}
  \partial_{t}u(x,t) = -u(x,t) + \int_{\RSet}w(x-y)f(u(y,t)) \,d y \qquad (x,t) \in
  \RSet \times \RSet_{>0}.
\end{equation}
Recall that for functions $u \in L^1(\RSet)$, the space of integrable functions on
$\RSet$, we define its Fourier Transform as
\[
  \hat u(\xi) = \int_{\RSet} u(x) e^{-i\xi x} \,d x, \qquad \xi \in \RSet.
\]
We henceforth assume that $w \in L^1(\RSet)$.

\section{Tutorial questions - nonlinear analysis}\label{sec:questions} 
\begin{question}\label{question:linearisation}
Let $u_*$ be a spatially homogeneous equilibrium
of the NFE \cref{eq:NF}. Setting $u(x,t) = u_* + v(x,t)$, and using a Taylor
expansion of $f$ show formally that small perturbations $v(x,t)$ to the homogeneous
steady state $u_*$ evolve according to the linear integro-differential equation
\begin{equation}\label{eq:linear}
  \partial_{t}v(x,t) = -v(x,t) + f'(u_*)\int_{\RSet}w(x-y)v(y,t) \,d y \qquad (x,t) \in \RSet \times \RSet_{>0}
\end{equation}
\end{question}

\begin{question}\label{eq:eigenvalues}
  Assuming $w$ is an even function, show that \cref{eq:linear} admits solutions of the form $v(x,t) = e^{\lambda t}
  e^{i\xi x}$ for any $(\xi,\lambda)$ satisfying the \textit{dispersion relation}
  \[
    \lambda(\xi) = -1 + f'(u_*) \hat w(\xi), \qquad \xi \in \RSet,
  \]
  and deduce that if $\lambda(\xi) < 0$ for all $\xi \in \RSet$, then $u_*$ is
  linearly stable to perturbations $v(x,0) = e^{i\xi x}$.
\end{question}

\begin{question}\label{question:kernels}
  Consider the NFE \cref{eq:NF} with synaptic kernel and firing rate given by
  \begin{equation}\label{eq:kernels and data}
    w(x) = A W(x), \qquad 
    W(x) = \frac{1}{\sqrt{\pi}}e^{-x^2} - \frac{1}{\sigma\sqrt{\pi}}e^{-x^2/\sigma^2},
    \qquad 
     f(u) = \frac{1}{1+e^{-\mu u+\theta}} - \frac{1}{1+e^{\theta}},
  \end{equation}
  respectively, where $A \in \RSet_{ \geq 0}$, $\sigma \in \RSet_{>1}$, $\mu \in \RSet_{>0}$ and
  $\theta \in \RSet$. Plot these functions for $A = 1$, $\sigma = 1.5$, $\mu = 10$,
  and $\theta = 0.5$. Discuss whether the synaptic
  kernel models excitation, inhibition, or both. Perturb the three parameters to see
  the effect they have on the functions. In preparation for the upcoming question,
  show that for arbitrary values $A, \sigma, \mu, \theta$, the kernel is
  \textit{balanced}, that is,
  \[
   \int_{\RSet} w(x) \,d x  = 0.
  \]
\end{question}

  % You may also want to compute the Fourier Transform $\hat w$ of $w$, and the
  % derivative $f'$ for arbitrary values $A, \sigma, \mu, \theta$.

\begin{question} 
  Show that the NFE \cref{eq:NF} with synaptic kernel and firing rate given by
  \cref{eq:kernels and data} admits the trivial steady state $u(x,t) \equiv 0$ for any values
  of the parameters $A, \sigma, \mu, \theta$. 

  Plot the Fourier Transform of $W$
  \[
    \hat W(\xi) = e^{-\xi^2/4} -  e^{-\sigma^2\xi^2/4}, \xi \in \RSet,
  \]
  for the parameters given in \cref{question:kernels}. Show that $\hat W$ admits
  global maxima at $\xi = \pm \xi_c$, with $\xi_c = \sqrt{8
  \ln(\sigma)/(\sigma^2-1)}$ and $\hat W_c := \hat W(\pm \xi_c)$.

  Using the result in \cref{question:linearisation} deduce that, if
  the coupling parameter is sufficiently large,
  \begin{equation}\label{eq:ACrit}
    A > \frac{1}{\hat W_c f'(0)}=:A_c,
  \end{equation}
  the trivial steady state $u(x,t) \equiv 0$ is linearly unstable to perturbations $e^{i\xi x}$,
  for $\pm \xi$ in an interval including $\xi_c$. At $A=A_c$ the system undergoes a
  Turing-like bifurcation. In an experiment where $A$ is set slightly higher than
  $A_c$, we expect that spatially-periodic patterns with wavelength $\xi_c$ emerge
  (albeit this may occur transiently).
\end{question} 

\section{Tutorial questions - numerical experiment}\label{sec:numerics} 
  In this section we will produce numerical evidence of the
  Turing-like bifurcation discussed in \cref{sec:questions}, and perform a time
  simulation of a neural field equation. Instead of posing the
  neural field an unbounded cortex, we shall consider a neural field equation posed
  on a large but finite cortex $D$. More specifically, we will take 
  $D = \RSet\setminus 2L \ZSet$, that is, a ring of width $2L$ with $L$ large, much
  larger than the characteristic timescale $\sigma$ of the synaptic kernel.
  To fix the ideas, one could set $D = [-L,L)$, and identify $L$ and $-L$. For PDEs,
  one would naturally speak about \textit{periodic boundary conditions}, but this is
  misleading for neural field equations, for which boundary conditions need not be
  specified. The system reads
  \begin{equation}\label{eq:NFPer}
    \begin{aligned}
    & \partial_{t}u(x,t) = -u(x,t) + \int_{D}w_p(x-y)f(u(y,t)) \,d y, \qquad (x,t) \in
    D \times [0,T] \\
    & u(x,0) = \varphi(x),
    \end{aligned}
  \end{equation}
  where $f$ is given in \cref{eq:kernels and data}, and $w_p$ is the $2L$-periodic
  extension of the function $w$ in \cref{eq:kernels and data}, that is, a
  $2L$-periodic function such that $w_p(x) = w(x)$ for all $x \in [-L,L)$.

  We discretise the cortex $D$ using $n$ evenly spaced points given by
  \[
    x_j = -L + (j-1)h, \qquad j = 1,\ldots,n, \qquad h = 2L/n.
  \]
  Henceforth we shall assume that the integer $n$ is \textit{even}. The grid size $h$
  is such that the closed interval $[-L,L]$ is split into
  $n$ equal strips and $n+1$ evenly-spaced points, of which the first $n$ are taken to
  discretise $[-L,L)$. This slightly awkward indexing is common when dealing with
  periodic functions: we approximate $u(x,t)$ only for $x \in [-L,L)$ and $u(L,t)$ is
  redundant, because it is equal to $u(-L,t)$. 

  A numerical scheme for approximating \cref{eq:NFPer} is heuristically obtained as
  follows: we evaluate the system at the node set $\{ x_i \}_{i=1}^{n}$, to obtain
  \[ %\label{eq:}
    \begin{aligned}
    & \partial_{t}u(x_i,t) = -u(x_i,t) + \int_{-L}^L w_p(x_i-y)f(u(y,t)) \,d y, 
    && i =1,\ldots,n, \quad t \in [0,T], \\
    & u(x_i,0) = \varphi(x_i),
    && i = 1,\ldots,n
    \end{aligned}
  \]
  and approximate the integral in the variable $y$ using the composite trapezium
  rule with $n$ strips. For a $2L$-periodic function $g$ the rule is given by
  \[
    \int_{-L}^{L} g(y) \,d y = g(x_1) \frac{h}{2} + \sum_{j=2}^{n} g(x_j)h +
    g(x_{n+1}) \frac{h}{2} = \sum_{j=1}^{n} g(x_j)h,
  \]
  from which we get confirmation that only function evaluations at nodes $\{
  x_i\}_{i=1}^{n}$ are required.

  One can show that the scheme above approximates \cref{eq:NFPer} via the system $n$ ODEs
  \begin{equation}\label{eq:ODE}
    U'(t) = -U(t) + A M F(U), \qquad t \in [0,T],  \qquad U(0) = \Phi,
  \end{equation}
  where $U(t) \in \RSet^n$ for all $t \in [0,T]$ is a vector with components $U_i(t)
  \approx u(x_i,t)$, the vector $\Phi \in \RSet^n$ has components $\Phi_i = \varphi(x_i)$, and the
  nonlinear function $F \colon \RSet^n \to \RSet^n$, has components $(F(U))_i = f(u_i)$.
  Finally the matrix $M \in \RSet^{n \times n}$ is expressed by setting $W_i = W(x_i)$ and 
  \[
    \kbordermatrix{
             % & x_1 = -L    & x_2 = -L+h     & \cdots    &   x_{n/2+1}=0  & \cdots    &  x_{n} = L-h \\
 % x_1 = -L    & h w_{n/2+1} & h w_{n/2+2}    & \cdots    &  hw_1          & \cdots    & hw_{n/2}  \\
 % x_2 = -L+h  & h w_{n/2}   & h w_{n/2+1}    & \cdots    &  hw_{n}        & \cdots    & hw_{n/2-1} \\
  % \vdots     & \vdots      & \vdots         & \cdots    &   \vdots       & \cdots    &  \vdots    \\
 % x_{n/2+1}=0 & h w_1       & h w_2          &  \cdots   &  hw_{n/2+1}    & \cdots    & hw_n       \\
  % \vdots     & \vdots      & \vdots         & \cdots    &   \vdots       & \cdots    &  \vdots    \\
 % x_{n} = L-h & h w_{n/2+2} & h w_{n/2+3}    &           &  hw_2          & \cdots    & hw_{n/2+1} \\
             & 1           & 2              & \cdots    &  n/2+1         & \cdots    &  n       \\
 1       & h W_{n/2+1} & h W_{n/2+2}    & \cdots    &  hW_1          & \cdots    & hW_{n/2}  \\
 2       & h W_{n/2}   & h W_{n/2+1}    & \cdots    &  hW_{n}        & \cdots    & hW_{n/2-1} \\
 \vdots  & \vdots      & \vdots         & \cdots    &   \vdots       & \cdots    &  \vdots    \\
 n/2     & h W_2       & h W_3          &  \cdots   &  hW_{n/2+2}    & \cdots    & hW_1       \\
 \color{red1}{n/2+1}   &  \color{red1}{h W_1}       & \color{red1}{h W_2}          & \color{red1}{\cdots}   &  \color{red1}{hW_{n/2+1}}    & \color{red1}{\cdots}    & \color{red1}{hW_n}       \\
 n/2+2   & h W_n       & h W_1          &  \cdots   &  hW_{n/2+3}    & \cdots    & hW_{n-1}    \\
 \vdots  & \vdots      & \vdots         & \cdots    &   \vdots       & \cdots    &  \vdots    \\
 n       & h W_{n/2+2} & h W_{n/2+3}    &           &  hW_2          & \cdots    & hW_{n/2+1} \\
}=M.
\]
One way to navigate the \textit{circulant} matrix $M$ and to relate \cref{eq:ODE} to
\cref{eq:NFPer} is to
recall that the grid has nodes $x_1=-L$, $x_{n/2+1} = 0$, and $x_{n}=L-h$. The
equation
\[
 \partial_{t}u(0,t) = -u(0,t) + \int_{-L}^Lw_p(0-y)f(u(y,t))\, dy,
\]
for instance, is approximated by the ODE
\[
  U_{n/2+1}'(t) = -U_{n/2+2} + h \sum_{j=1}^n w(-x_j) f(U_j) h
                = -U_{n/2+1} + Ah \sum_{j=1}^n W_jf(U_j) h,
\]
and the corresponding row of the matrix $M$ is highlighted in red. Similarly
\[
 \partial_{t}u(h,t) = -u(h,t) + \int_{-L}^Lw_p(h-y)f(u(y,t))\, dy,
\]
is approximated by
\[
  U_{n/2+2}'(t) = -U_{n/2+2} + h \sum_{j=1}^n w(h-x_j) f(U_j) h
                = -U_{n/2+2} + h \sum_{j=1}^n w(x_{j-1}) f(U_j) h,
\]
which, upon recalling that $x_0=x_n$ by periodicity, gives row $n/2+2$ of the matrix.
The fact that $M$ is circulant descends from the integral in \cref{eq:NFPer} being
a circular convolution, a convolution between periodic functions with the same period.

\begin{question} \label{question:codeMatrix}
  We will now work towards the construction of code to timestep the neural field
  equation. Assume the following setup: $A = 1$, $\sigma=1.5$, $\mu =10$,
  $\theta = 0.5$, $L = 10\pi$, $n = 2^{10}$.

  Write a code that, for generic number of nodes $n$, stores the nodes
  $\{x_j\}_{j=1}^n$ in a vector, and forms the $n$-by-$n$ matrix $M$. You may want to
  test the matrix $M$ for low $n$, and known values of $w$. Ultimately you can
  download a file containing the matrix for the parameter above at this link
  \da[]{Provide link}
\end{question}

\begin{question} 
  Use the matrix $M$ in \cref{question:codeMatrix} to time step the set of ODEs
  \cref{eq:ODE}. You can use any off-the-shelf timestepper available on your platform
  for time step, or write your own. In the solutions I have used Matlab's in-built
  \texttt{ode45} which is an explicit $4$th order, time-adaptive scheme.
\end{question}

\begin{question} 
  Produce numerical evidence that, with the parameters given in
  \cref{question:codeMatrix}, the trivial steady state $u(x,t) \equiv 0$ is linearly
  stable. To do this, you can set the initial condition $\Phi$ to be a random vector
  with small norm (which models small random perturbations around $0$), and observe the
  initial perturbations decay.
\end{question}

\begin{question} 
  From \cref{sec:questions} we know that, upon increasing $A$ above a critical value
  $A_c$, we should reach a Turing-like bifurcation. Calculate $A_c$ from
  \cref{eq:ACrit}, set $A > A_c$ and repeat the numerical simulation: you should see
  the formation of patterns.
\end{question}

\begin{question} 
  From \cref{sec:questions} we know that emerging patterns should have a specific wavelength
  $\xi_c$. Verify that, when $A > A_c$, patterns are formed at the predicted
  wavelength $\xi_c$. One way to do that is to set a deterministic initial condition with
  wavelength $\xi_c$, for instance $\varphi(x) = \eps \cos(\xi_c\, x)$ for $\eps \ll 1$, instead of a random
  one, and observe the perturbations decay or amplify when $A$ is above or below the
  critical value, respectively.
\end{question}

\begin{question} 
  As for ODEs, bifurcations of stationary states in a neural field equation can be
  super- or sub-critical. The analysis of the previous section does not address the
  criticality of the Turing-like bifurcation, but we can do so numerically. One
  possibility is to time-step the system for various values $\{A_k\}$ in the
  interval $[1,3] \ni A_c$, keeping an identical initial condition in the form of a small
  perturbation of the trivial state, say $\varphi(x) = \eps \cos(\xi_c \, x)$. 

  One observation is that $T$ must be large enough that each simulation gets close to
  equilibrium, either the trivial state (when $A < A_c$) or the patterned state
  (when $A > A_c$).

  At the end of the simulation for $A = A_k$, we record a solution measure of the
  corresponding final state $u_k(x,T)$, for instance the infinity norm  $\|
  u_k(\blank,T) \|_{\infty} = \max_{x \in [-L,L]} | u_k(x,T) |$. 

  We plot the set of points with coordinates $(A_k, \| u_k(\blank, T) \|_{\infty})$
  to obtain a crude bifurcation diagram, displaying branches of stable steady states
  as $A$ varies in $[1,3]$. Is the Turing-like bifurcation sub- or super-critical?
\end{question}

\begin{question}
  The most expensive operation when timestepping a neural field is the matrix-vector
  multiplication featuring in the right-hand side of \cref{eq:ODE}. The $n$-by-$n$
  matrix $M$ is full, and multiplying it to the right by an $n$-vector takes $O(n^2)$
  operations. This becomes expensive on large-scale computations.

  When the matrix $M$ is circulant, one can use Fast Fourier Transforms (FFTs) to perform
  matrix-vector multiplications in $O(n \log n)$ operations. Amend your code to carry
  out the matrix-vector multiplication using FFTs, and compare its performance with
  the old code.
\end{question}


% \bibliographystyle{siamplain}
% \bibliography{references}
\end{document}
